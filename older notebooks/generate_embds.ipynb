{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WSC Project - Data Analysis & NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import ast\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# Data science imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP imports\n",
    "import nltk\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import json\n",
    "import os \n",
    "import random\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer, MultiLabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.svm import LinearSVC\n",
    "import tiktoken\n",
    "import pickle\n",
    "# import xgboost as xgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Jupyter settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings for full text\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = list(set(pd.read_csv(\"data/actions.csv\")[\"parameter\"]))\n",
    "pseudo_df = pd.read_csv(\"data/pseudo_actions_labels_with_id.csv\")[['sample_id', 'action_detected']]\n",
    "transcripts_folds_df = pd.read_csv('data/transcripts_folds.csv')\n",
    "features_df = transcripts_folds_df[['sample_id', 'Text', 'tokenized_text', 'events', 'actions_in_text', 'fold1', 'fold2', 'fold3', 'fold4', 'fold5']]\n",
    "# features_df = transcripts_folds_df[['sample_id', 'Text', 'tokenized_text', 'events', 'actions_in_text']]\n",
    "\n",
    "expr_df = pseudo_df.merge(features_df, on='sample_id', how='inner').rename(columns={'action_detected': 'actions_pseudo_label', 'actions_in_text': 'actions_str_detected', 'Text': 'transcript_text'})\n",
    "expr_df['actions_pseudo_label'] =  expr_df['actions_pseudo_label'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "with open('data/actions_processed_to_action.json', 'r') as f:\n",
    "    actions_map = json.load(f)\n",
    "\n",
    "inverse_actions_map = {v: k for k, v in actions_map.items()}\n",
    "expr_df['actions_str_detected'] = expr_df['actions_str_detected'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "expr_df['actions_str_detected'] = expr_df['actions_str_detected'].apply(lambda actions: [inverse_actions_map.get(a, a) for a in actions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "augmented_texts_df = pd.read_csv('data/augmented_texts_processed.csv')[['sample_id', 'augmented_text', 'tokenized_augmented_text', 'action']]\n",
    "\n",
    "def concat_augmentations_to_fold_df(fold_train_df):\n",
    "    augmented_texts_train = augmented_texts_df[augmented_texts_df['sample_id'].isin(fold_train_df['sample_id'])]\n",
    "    augmented_texts_train = augmented_texts_train.rename(columns={'augmented_text': 'transcript_text', 'action': 'actions_pseudo_label', 'tokenized_augmented_text': 'tokenized_text'})\n",
    "    augmented_texts_train['actions_pseudo_label'] = augmented_texts_train['actions_pseudo_label'].apply(lambda x: [x])\n",
    "    augmented_texts_train = augmented_texts_train.merge(\n",
    "        fold_train_df.drop(['actions_pseudo_label', 'transcript_text', 'tokenized_text'], axis=1), \n",
    "        on='sample_id', \n",
    "        how='inner'\n",
    "    )\n",
    "    return pd.concat([fold_train_df, augmented_texts_train], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUGMENT_DATA = True\n",
    "\n",
    "df = expr_df.copy()\n",
    "action_counts = df['actions_pseudo_label'].explode().value_counts()\n",
    "\n",
    "fold_idx = 1\n",
    "fold_col = f'fold{fold_idx}'\n",
    "fold_train_df = df[df[fold_col] == 'train']\n",
    "if AUGMENT_DATA:\n",
    "    fold_train_df = concat_augmentations_to_fold_df(fold_train_df)\n",
    "    fold_val_df = df[df[fold_col] == 'val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = fold_train_df.copy()\n",
    "train_df['events_str'] = ['Events: ' + ', '.join(l) for l in [[] if l == '[None]' else eval(l) for l in train_df['events'].tolist()]]\n",
    "train_df['events_and_transcript'] = train_df['events_str'] + '\\n Transcript: ' + train_df['transcript_text']\n",
    "# train_df['events_and_transcript'] =  train_df['transcript_text']\n",
    "\n",
    "val_df = fold_val_df.copy()\n",
    "# val_df['events_str'] = ['Events: ' + ', '.join(l) for l in [[] if l == '[None]' else eval(l) for l in val_df['events'].tolist()]]\n",
    "# val_df['events_and_transcript'] = val_df['events_str'] + '\\n Transcript: ' + val_df['transcript_text']\n",
    "val_df['events_and_transcript'] =  val_df['transcript_text']\n",
    "\n",
    "X_train = train_df['events_and_transcript'].tolist()\n",
    "y_train = fold_train_df['actions_pseudo_label'].tolist()\n",
    "\n",
    "X_val = val_df['events_and_transcript'].tolist()\n",
    "y_val = val_df['actions_pseudo_label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_COLLECTION = 'train'\n",
    "# INFERENCE_COLLECTION = 'val'\n",
    "\n",
    "EMBEDDINGS_TASK = 'detector'\n",
    "# EMBEDDINGS_TASK = 'validator'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = df.copy()\n",
    "all_df['events_str'] = ['Events: ' + ', '.join(l) for l in [[] if l == '[None]' else eval(l) for l in all_df['events'].tolist()]]\n",
    "all_df['events_and_transcript'] = all_df['events_str'] + '\\n Transcript: ' + all_df['transcript_text']\n",
    "all_df_text = all_df['transcript_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/70 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|██████████| 70/70 [4:47:52<00:00, 246.74s/it]    \n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "MODEL = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=MODEL, device=-1)\n",
    "\n",
    "batch_size = 16 # or any batch size you prefer\n",
    "actions_preds_all = []\n",
    "for i in tqdm(range(0, len(all_df_text), batch_size)):\n",
    "    batch = all_df_text[i:i+batch_size]\n",
    "    actions_preds_all.extend(classifier(batch, ACTIONS, multi_label=False))\n",
    "\n",
    "\n",
    "# Convert the list to a dict, using index as key\n",
    "actions_preds_val_dict = {i: v for i, v in enumerate(actions_preds_all)}\n",
    "with open('data/all_df_preds_DeBERTa-v3-base-mnli-fever-anli.json', 'w') as f:\n",
    "    json.dump(actions_preds_val_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = [(x, y, label) for x, y, label in zip(X_val, y_val, val_df['Label']) if len(y) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Angela Merkel is a politician in Germany and leader of the CDU',\n",
       " 'labels': ['side step',\n",
       "  'floater',\n",
       "  'post up',\n",
       "  'no look pass',\n",
       "  'alley oop',\n",
       "  'splash',\n",
       "  'poster',\n",
       "  'bank shot',\n",
       "  'slam dunk',\n",
       "  'tip in',\n",
       "  'fake',\n",
       "  'dime',\n",
       "  'nothing but net',\n",
       "  'lob',\n",
       "  'outlet pass',\n",
       "  'behind the back',\n",
       "  'step back',\n",
       "  'pick and roll',\n",
       "  'swish',\n",
       "  'give and go',\n",
       "  'reverse dunk',\n",
       "  'take it to the rack',\n",
       "  'fadeaway',\n",
       "  'pump fake',\n",
       "  'euro step',\n",
       "  'double team',\n",
       "  'between the legs',\n",
       "  'jab step',\n",
       "  'flop',\n",
       "  'backdoor',\n",
       "  'shake and bake',\n",
       "  'coast to coast',\n",
       "  'teardrop',\n",
       "  'baseball pass',\n",
       "  'rainbow shot',\n",
       "  'jam',\n",
       "  'finger roll',\n",
       "  'tomahawk'],\n",
       " 'scores': [0.02935909666121006,\n",
       "  0.027067378163337708,\n",
       "  0.023653825744986534,\n",
       "  0.014645302668213844,\n",
       "  0.013345428742468357,\n",
       "  0.010482476092875004,\n",
       "  0.0086736548691988,\n",
       "  0.0067649600096046925,\n",
       "  0.006156327668577433,\n",
       "  0.005655469838529825,\n",
       "  0.003958581481128931,\n",
       "  0.0034659183584153652,\n",
       "  0.0034551972057670355,\n",
       "  0.0024850498884916306,\n",
       "  0.00230234838090837,\n",
       "  0.00192182173486799,\n",
       "  0.0017853198805823922,\n",
       "  0.0016667887102812529,\n",
       "  0.0015677718911319971,\n",
       "  0.001502110273577273,\n",
       "  0.0014898581430315971,\n",
       "  0.0014159964630380273,\n",
       "  0.0011008308501914144,\n",
       "  0.0010694495867937803,\n",
       "  0.0009454888640902936,\n",
       "  0.0007543730898760259,\n",
       "  0.0007109310827217996,\n",
       "  0.0006689998554065824,\n",
       "  0.0004853819846175611,\n",
       "  0.00045836783829145133,\n",
       "  0.0002976174291688949,\n",
       "  0.00018320327217224985,\n",
       "  0.0001768903894117102,\n",
       "  0.00011522147542564198,\n",
       "  8.324950613314286e-05,\n",
       "  7.711353100603446e-05,\n",
       "  7.072389416862279e-05,\n",
       "  5.69743788219057e-05]}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premise = \"John Collins can't convert the lob inside, yesterday he slam dunked it\"\n",
    "# text = \"Angela Merkel is a politician in Germany and leader of the CDU\"\n",
    "hypothesis_template = \"This action mentioned in the commentary is {}\"\n",
    "classes_verbalized = ACTIONS\n",
    "zeroshot_classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/deberta-v3-large-zeroshot-v2.0\")  # change the model identifier here\n",
    "output = zeroshot_classifier(text, classes_verbalized, hypothesis_template=hypothesis_template, multi_label=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entailment': 90.8, 'not_entailment': 9.2}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"MoritzLaurer/DeBERTa-v3-small-mnli-fever-docnli-ling-2c\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# premise = \"I first thought that I liked the movie, but upon second thought it was actually disappointing.\"\n",
    "# hypothesis = \"The movie was good.\"\n",
    "\n",
    "\n",
    "\n",
    "# model_name = \"MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# premise = \"I first thought that I liked the movie, but upon second thought it was actually disappointing.\"\n",
    "# hypothesis = \"The movie was good.\"\n",
    "\n",
    "# premise = \"'Jordan Posterized Oxymora!'\"\n",
    "premise = \"John Collins can't convert the lob inside, yesterday he slam dunked it\"\n",
    "# hypothesis = \"A poster action is mentioned and executed in this current play.\"\n",
    "# hypothesis = \"A slam dunked action is mentioned but executed sometime else\" \n",
    "hypothesis = \"A slam dunked action is mentioned and executed\" \n",
    "# hypothesis = \"A poster action is mentioned and executed\" \n",
    "\n",
    "\n",
    "input = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "output = model(input[\"input_ids\"])  # device = \"cuda:0\" or \"cpu\"\n",
    "prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "label_names = [\"entailment\", \"not_entailment\"]\n",
    "prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deroberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL = \"microsoft/deberta-v3-base\"\n",
    "MODEL = \"MoritzLaurer/DeBERTa‑v3‑base‑mnli\"\n",
    "\n",
    "# MODEL = \"microsoft/SportsBERT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: 'MoritzLaurer/DeBERTa‑v3‑base‑mnli'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/projects/WSCsports/wsc project/wsc_venv/lib/python3.9/site-packages/transformers/utils/hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/projects/WSCsports/wsc project/wsc_venv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/projects/WSCsports/wsc project/wsc_venv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:160\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'MoritzLaurer/DeBERTa‑v3‑base‑mnli'.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m     H \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m action: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m occurs in this play.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m set_deterministic(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m tok\u001b[38;5;241m=\u001b[39m\u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# DeBERTa does not have .to() for tokenizer, so skip moving tokenizer to device\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tok\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: tok\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tok\u001b[38;5;241m.\u001b[39meos_token \u001b[38;5;129;01mor\u001b[39;00m tok\u001b[38;5;241m.\u001b[39munk_token\n",
      "File \u001b[0;32m~/Documents/projects/WSCsports/wsc project/wsc_venv/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:834\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    833\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 834\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    836\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/projects/WSCsports/wsc project/wsc_venv/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:666\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    663\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[1;32m    665\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 666\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    683\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/projects/WSCsports/wsc project/wsc_venv/lib/python3.9/site-packages/transformers/utils/hub.py:466\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    468\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: 'MoritzLaurer/DeBERTa‑v3‑base‑mnli'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "DEVICE = 'cpu'\n",
    "\n",
    "def set_deterministic(s=42):\n",
    "    os.environ[\"PYTHONHASHSEED\"]=str(s); random.seed(s); np.random.seed(s); torch.manual_seed(s)\n",
    "    try: torch.use_deterministic_algorithms(True)\n",
    "    except: pass\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "def build_train_pairs(X,Y,A):\n",
    "    T,A2,L=[],[],[]\n",
    "    for t,ys in zip(X,Y):\n",
    "        P=list(set(ys)); N=[a for a in A if a not in P]; k=max(len(ys),len(P))\n",
    "        if N: N=rng.choice(N,size=min(k,len(N)),replace=False).tolist()\n",
    "        for a in P: T.append(t); A2.append(H(a)); L.append(1.)\n",
    "        for a in N: T.append(t); A2.append(H(a)); L.append(0.)\n",
    "    return T,A2,torch.tensor(L,dtype=torch.float32)\n",
    "\n",
    "def build_val_pairs(X,Y,A):\n",
    "    T,A2,L=[],[],[]\n",
    "    for t,ys in zip(X,Y):\n",
    "        S=set(ys)\n",
    "        for a in A:\n",
    "            T.append(t)\n",
    "            A2.append(H(a))\n",
    "            L.append(1. if a in S else 0.)\n",
    "    return T,A2,torch.tensor(L,dtype=torch.float32)\n",
    "\n",
    "\n",
    "class Pairs(torch.utils.data.Dataset):\n",
    "    def __init__(self,tok,T,A,L,m=160): self.tok,tok.model_max_length,self.T,self.A,self.L,self.m=tok,m,T,A,L,m\n",
    "    def __len__(self): return len(self.T)\n",
    "    def __getitem__(self,i):\n",
    "        enc=self.tok(self.T[i], self.A[i], truncation=True, padding=\"max_length\", max_length=self.m)\n",
    "        # DeBERTa does not use token_type_ids, so always remove if present\n",
    "        enc.pop(\"token_type_ids\", None)\n",
    "        enc={k:torch.tensor(v) for k,v in enc.items()}; enc[\"labels\"]=self.L[i]; return enc\n",
    "\n",
    "\n",
    "if EMBEDDINGS_TASK == 'detector':\n",
    "    H = lambda action: f\"This commentary mentions {action}.\"\n",
    "elif EMBEDDINGS_TASK == 'validator':\n",
    "    H = lambda action: f\"A {action} occurs in this play.\"\n",
    "\n",
    "set_deterministic(42)\n",
    "tok=AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
    "# DeBERTa does not have .to() for tokenizer, so skip moving tokenizer to device\n",
    "if tok.pad_token is None: tok.pad_token = tok.eos_token or tok.unk_token\n",
    "\n",
    "Ttr,Atr,Ltr=build_train_pairs(X_train,y_train,ACTIONS)\n",
    "Tva,Ava,Lva=build_val_pairs(X_val,y_val,ACTIONS)\n",
    "ds_tr,ds_va=Pairs(tok,Ttr,Atr,Ltr),Pairs(tok,Tva,Ava,Lva)\n",
    "\n",
    "enc=AutoModel.from_pretrained(MODEL)\n",
    "for p in enc.parameters(): p.requires_grad=False\n",
    "enc = enc.to(DEVICE)\n",
    "\n",
    "print('Train trainscripts data', len(X_train))\n",
    "print('Val trainscripts data', len(X_val))\n",
    "print('Train data points', len(Ttr))\n",
    "print('Val data points', len(Tva))\n",
    "\n",
    "\n",
    "if INFERENCE_COLLECTION =='train':\n",
    "    x_text = Ttr\n",
    "    x_acts = Atr\n",
    "    X_binary_label = Ltr\n",
    "\n",
    "if INFERENCE_COLLECTION =='val':\n",
    "    x_text = Tva\n",
    "    x_acts = Ava\n",
    "    X_binary_label = Lva\n",
    "\n",
    "print(INFERENCE_COLLECTION)\n",
    "print(len(x_text))\n",
    "mean_pooling_embs = []\n",
    "cls_embs = []\n",
    "bs = 64\n",
    "enc.eval()\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(x_text), bs)):\n",
    "        print(f\"{(i/len(x_text)*100):.1f}%\")\n",
    "        t_batch = x_text[i:i+bs]\n",
    "        a_batch = x_acts[i:i+bs]\n",
    "        batch = tok(t_batch, a_batch, truncation=True, padding=\"max_length\", max_length=160, return_tensors=\"pt\")\n",
    "        # DeBERTa does not use token_type_ids, so always remove if present\n",
    "        batch.pop(\"token_type_ids\", None)\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        out = enc(**batch)\n",
    "        \n",
    "        attn_mask = batch['attention_mask'].unsqueeze(-1)\n",
    "        sum_hidden = (out.last_hidden_state * attn_mask).sum(1)\n",
    "        lens = attn_mask.sum(1).clamp(min=1)\n",
    "        mean_pooled = sum_hidden / lens\n",
    "        mean_pooling_embs.append(mean_pooled.detach().cpu())\n",
    "        cls_embs.append(out.last_hidden_state[:,0].detach().cpu())\n",
    "\n",
    "cls_embs = torch.cat(cls_embs)\n",
    "mean_pooling_embs = torch.cat(mean_pooling_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SportsBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE = 'cpu'\n",
    "\n",
    "# def set_deterministic(s=42):\n",
    "#     os.environ[\"PYTHONHASHSEED\"]=str(s); random.seed(s); np.random.seed(s); torch.manual_seed(s)\n",
    "#     try: torch.use_deterministic_algorithms(True)\n",
    "#     except: pass\n",
    "\n",
    "# rng = np.random.default_rng(42)\n",
    "\n",
    "# def build_train_pairs(X,Y,A):\n",
    "#     T,A2,L=[],[],[]\n",
    "#     for t,ys in zip(X,Y):\n",
    "#         P=list(set(ys)); N=[a for a in A if a not in P]; k=max(len(ys),len(P))\n",
    "#         if N: N=rng.choice(N,size=min(k,len(N)),replace=False).tolist()\n",
    "#         for a in P: T.append(t); A2.append(H(a)); L.append(1.)\n",
    "#         for a in N: T.append(t); A2.append(H(a)); L.append(0.)\n",
    "#     return T,A2,torch.tensor(L,dtype=torch.float32)\n",
    "\n",
    "# def build_val_pairs(X,Y,A):\n",
    "#     T,A2,L=[],[],[]\n",
    "#     for t,ys in zip(X,Y):\n",
    "#         S=set(ys)\n",
    "#         for a in A:\n",
    "#             T.append(t)\n",
    "#             A2.append(H(a))\n",
    "#             L.append(1. if a in S else 0.)\n",
    "#     return T,A2,torch.tensor(L,dtype=torch.float32)\n",
    "\n",
    "\n",
    "# class Pairs(torch.utils.data.Dataset):\n",
    "#     def __init__(self,tok,T,A,L,m=160): self.tok,tok.model_max_length,self.T,self.A,self.L,self.m=tok,m,T,A,L,m\n",
    "#     def __len__(self): return len(self.T)\n",
    "#     def __getitem__(self,i):\n",
    "#         enc=self.tok(self.T[i], self.A[i], truncation=True, padding=\"max_length\", max_length=self.m)\n",
    "#         # --- drop token_type_ids for single-segment models ---\n",
    "#         enc.pop(\"token_type_ids\", None)  # <<< changed\n",
    "#         enc={k:torch.tensor(v) for k,v in enc.items()}; enc[\"labels\"]=self.L[i]; return enc\n",
    "\n",
    "# MODEL, REV = \"microsoft/SportsBERT\", \"refs/pr/4\"\n",
    "# if EMBEDDINGS_TASK == 'detector':\n",
    "#     H = lambda action: f\"This commentary mentions {action}.\"\n",
    "# elif EMBEDDINGS_TASK == 'validator':\n",
    "#     H = lambda action: f\"A {action} occurs in this play.\"\n",
    "\n",
    "# set_deterministic(42)\n",
    "# tok=AutoTokenizer.from_pretrained(MODEL, revision=REV, use_fast=True)\n",
    "# tok = tok.to(DEVICE) if hasattr(tok, 'to') else tok\n",
    "# if tok.pad_token is None: tok.pad_token = tok.eos_token or tok.unk_token\n",
    "\n",
    "# Ttr,Atr,Ltr=build_train_pairs(X_train,y_train,ACTIONS)\n",
    "# Tva,Ava,Lva=build_val_pairs(X_val,y_val,ACTIONS)\n",
    "# ds_tr,ds_va=Pairs(tok,Ttr,Atr,Ltr),Pairs(tok,Tva,Ava,Lva)\n",
    "\n",
    "# enc=AutoModel.from_pretrained(MODEL, revision=REV)\n",
    "# for p in enc.parameters(): p.requires_grad=False\n",
    "# enc = enc.to(DEVICE)\n",
    "\n",
    "# print('Train trainscripts data', len(X_train))\n",
    "# print('Val trainscripts data', len(X_val_sample))\n",
    "# print('Train data points', len(Ttr))\n",
    "# print('Val data points', len(Tva))\n",
    "\n",
    "\n",
    "# if INFERENCE_COLLECTION =='train':\n",
    "#     x_text = Ttr\n",
    "#     x_acts = Atr\n",
    "#     X_binary_label = Ltr\n",
    "\n",
    "# if INFERENCE_COLLECTION =='val':\n",
    "#     x_text = Tva\n",
    "#     x_acts = Ava\n",
    "#     X_binary_label = Lva\n",
    "\n",
    "# print(INFERENCE_COLLECTION)\n",
    "# print(len(x_text))\n",
    "# mean_pooling_embs = []\n",
    "# cls_embs = []\n",
    "# bs = 64\n",
    "# enc.eval()\n",
    "# with torch.no_grad():\n",
    "#     for i in tqdm(range(0, len(x_text), bs)):\n",
    "#         print(f\"{(i/len(x_text)*100):.1f}%\")\n",
    "#         t_batch = x_text[i:i+bs]\n",
    "#         a_batch = x_acts[i:i+bs]\n",
    "#         batch = tok(t_batch, a_batch, truncation=True, padding=\"max_length\", max_length=160, return_tensors=\"pt\")\n",
    "#         # --- drop token_type_ids when the model can't use them (avoids IndexError) ---\n",
    "#         if getattr(enc.config, \"type_vocab_size\", 2) <= 1 and \"token_type_ids\" in batch:  # <<< changed\n",
    "#             batch.pop(\"token_type_ids\")                                             # <<< changed\n",
    "#         batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "#         out = enc(**batch)\n",
    "        \n",
    "#         attn_mask = batch['attention_mask'].unsqueeze(-1)\n",
    "#         sum_hidden = (out.last_hidden_state * attn_mask).sum(1)\n",
    "#         lens = attn_mask.sum(1).clamp(min=1)  # <<< changed (safety vs. division by zero)\n",
    "#         mean_pooled = sum_hidden / lens\n",
    "#         mean_pooling_embs.append(mean_pooled.detach().cpu())\n",
    "#         cls_embs.append(out.last_hidden_state[:,0].detach().cpu())\n",
    "\n",
    "# cls_embs = torch.cat(cls_embs)\n",
    "# mean_pooling_embs = torch.cat(mean_pooling_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save Embds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITE = False\n",
    "\n",
    "def save_embeddings(INFERENCE_COLLECTION, x_acts, cls_embs, mean_pooling_embs):\n",
    "    # Ensure the output directory exists\n",
    "    out_dir = \"data/embds\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare data to save\n",
    "    embd_data = {\n",
    "        \"INFERENCE_COLLECTION\": INFERENCE_COLLECTION,\n",
    "        \"actions_labels\": [s.replace(\"This commentary mentions \", \"\").rstrip(\".\") for s in x_acts],\n",
    "        \"cls_embs\": cls_embs.cpu(),  # ensure on CPU\n",
    "        \"mean_pooling_embs\": mean_pooling_embs.cpu(),\n",
    "        \"sample_label\": X_binary_label.cpu()\n",
    "\n",
    "\n",
    "    }\n",
    "\n",
    "    # Save as .pt (PyTorch native)\n",
    "    torch.save(embd_data, os.path.join(out_dir, f\"{INFERENCE_COLLECTION}_embds_{EMBEDDINGS_TASK}.pt\"))\n",
    "\n",
    "    # For pickle, better to convert to numpy for portability\n",
    "    embd_data_np = {**embd_data,\n",
    "                    \"cls_embs\": cls_embs.cpu().numpy(),\n",
    "                    \"mean_pooling_embs\": mean_pooling_embs.cpu().numpy()}\n",
    "\n",
    "    with open(os.path.join(out_dir, f\"{INFERENCE_COLLECTION}_embds_{EMBEDDINGS_TASK}.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(embd_data_np, f)\n",
    "\n",
    "if WRITE:\n",
    "    save_embeddings(INFERENCE_COLLECTION, x_acts, cls_embs, mean_pooling_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(INFERENCE_COLLECTION, embeddings_task, filetype=\"pt\"):\n",
    "    p = f\"data/embds/{INFERENCE_COLLECTION}_embds_{embeddings_task}.{filetype}\"\n",
    "    if filetype == \"pt\":\n",
    "        return torch.load(p)\n",
    "    if filetype == \"pkl\":\n",
    "        with open(p, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    raise ValueError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsc_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
