{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E2E Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data science imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import pandas as pd\n",
    "import ast\n",
    "import spacy\n",
    "import json\n",
    "\n",
    "# NLP imports\n",
    "import nltk\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_df = pd.read_csv('data/transcripts.csv').drop_duplicates(ignore_index=True)\n",
    "transcripts_df.insert(0, 'sample_id', range(1, len(transcripts_df) + 1))\n",
    "ACTIONS = list(set(pd.read_csv(\"data/actions.csv\")[\"parameter\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pseudo_df = pd.read_csv(\"data/pseudo_actions_labels_with_id.csv\")[['sample_id', 'action_detected']]\n",
    "augmented_texts_df = pd.read_csv('data/augmented_texts_processed.csv')[['sample_id', 'augmented_text', 'tokenized_augmented_text', 'action']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENT_TYPES = ['Missed Shot', '2-pointer', '3-pointer', 'Turnover', 'Rebound', 'Dunk', 'Foul', 'Assist', 'Steal', 'Jump Ball', '2-pts Made', 'FT-Made', '3-pts Made', 'Quarter End', 'FT-Missed', 'Block']\n",
    "event_types_pattern = r\"(?i)(\" + \"|\".join(map(re.escape, sorted(EVENT_TYPES, key=len, reverse=True))) + r\")\\s+by\\s+\"\n",
    "START = re.compile(event_types_pattern)\n",
    "PAIR  = re.compile(r\"(?i)^\\s*(?P<a>\" + \"|\".join(map(re.escape, sorted(EVENT_TYPES, key=len, reverse=True))) + r\")\\s+by\\s+(?P<p>.+?)\\s*$\")\n",
    "\n",
    "def split_events(s):\n",
    "    s = re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
    "    idx = [m.start() for m in START.finditer(s)]\n",
    "    if not idx: return [s] if s else []\n",
    "    idx.append(len(s))\n",
    "    return [re.sub(r\"\\s+\", \" \", s[idx[i]:idx[i+1]].strip()) for i in range(len(idx)-1)]\n",
    "\n",
    "def split_action_player(e):\n",
    "    m = PAIR.match(re.sub(r\"\\s+\", \" \", e).strip())\n",
    "    return (m.group(\"a\").strip(), m.group(\"p\").strip()) if m else (None, None)\n",
    "\n",
    "def clean_events(transcripts_df):\n",
    "    transcripts_df = transcripts_df.copy()\n",
    "    event_split = transcripts_df['EventName'].str.split('by')\n",
    "    event_types = event_split.str[0].str.strip()\n",
    "\n",
    "    \n",
    "\n",
    "    transcripts_df[\"event_list\"] = transcripts_df[\"EventName\"].apply(split_events)\n",
    "    events_players = transcripts_df[\"event_list\"].apply(lambda lst: [split_action_player(e) for e in lst])\n",
    "    transcripts_df[\"players\"] = events_players.apply(lambda lst: [ap[1] for ap in lst])\n",
    "    transcripts_df[\"events\"] = events_players.apply(lambda lst: [ap[0] for ap in lst])\n",
    "    transcripts_df[\"EventNameCleaned\"] = transcripts_df[\"event_list\"].apply(lambda x: ', '.join(x))\n",
    "    return transcripts_df.drop(columns=['event_list'])\n",
    "\n",
    "\n",
    "# spaCy's stopword set\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "STOPWORDS = nlp.Defaults.stop_words\n",
    "\n",
    "# Broad temporal concept vocabulary\n",
    "temporal_candidates = {\n",
    "    # basic temporal connectives\n",
    "    \"after\", \"before\", \"until\", \"till\", \"since\", \"when\", \"while\", \"once\", \"then\", \"later\", \"earlier\",\n",
    "    \"eventually\", \"soon\", \"previously\", \"recently\", \"now\",\n",
    "    # specific time references\n",
    "    \"today\", \"tomorrow\", \"yesterday\", \"tonight\", \"morning\", \"afternoon\", \"evening\",\n",
    "    \"day\", \"week\", \"month\", \"year\", \"season\", \"period\", \"half\", \"quarter\",\n",
    "    # sequence/order terms\n",
    "    \"final\", \"first\", \"second\", \"third\", \"last\", \"next\"\n",
    "}\n",
    "\n",
    "# Broad negation vocabulary\n",
    "negation_candidates = {\n",
    "    \"no\", \"not\", \"n't\", \"never\", \"cannot\", \"can't\", \"nobody\", \"none\", \"nothing\", \"nowhere\",\n",
    "    \"neither\", \"nor\", \"without\", \"minus\"\n",
    "}\n",
    "\n",
    "# Intersections with spaCy's stopword list\n",
    "TEMPORAL_STOPWORDS = sorted({w for w in STOPWORDS if w in temporal_candidates})\n",
    "NEGATION_STOPWORDS = sorted({w for w in STOPWORDS if w in negation_candidates})\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Lower, lemmatize, remove punct/space\"\"\"\n",
    "    return [t.lemma_.lower() for t in nlp(text, disable=[\"parser\", \"ner\"]) if not (t.is_punct or t.is_space)]\n",
    "\n",
    "def lemmatize(text, phrases_patterns):\n",
    "    text_tokens = preprocess_text(text)\n",
    "    result = text_tokens[:]\n",
    "    for pattern in phrases_patterns:\n",
    "        pattern_split = pattern.split('_')\n",
    "        if len(pattern_split) > 1:\n",
    "            for i in range(len(result) - len(pattern_split) + 1):\n",
    "                if result[i:i+len(pattern_split)] == pattern_split:\n",
    "                    result = result[:i] + [pattern] + result[i+len(pattern_split):]\n",
    "                    break\n",
    "    result = [t for t in result if not nlp.vocab[t].is_stop or t in TEMPORAL_STOPWORDS + NEGATION_STOPWORDS]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENTS = list(set([event for sublist in clean_events(transcripts_df)['events'].tolist() for event in sublist if event is not None]))\n",
    "ACTIONS_PROCESSED = ['_'.join(preprocess_text(action)) for action in ACTIONS]\n",
    "EVENTS_PROCESSED = ['_'.join(preprocess_text(event)) for event in EVENTS]\n",
    "phrases_patterns = ACTIONS_PROCESSED + EVENTS_PROCESSED\n",
    "\n",
    "def extract_actions_from_lemmatized(lemmatized_tokens):\n",
    "    return [action for action in ACTIONS_PROCESSED if action in lemmatized_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detector code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def concat_augmentations_to_fold_df(transcripts_df):\n",
    "    augmented_texts_train = augmented_texts_df[augmented_texts_df['sample_id'].isin(transcripts_df['sample_id'])]\n",
    "    augmented_texts_train = augmented_texts_train.rename(columns={'augmented_text': 'transcript_text', 'action': 'actions_pseudo_label', 'tokenized_augmented_text': 'tokenized_text'})\n",
    "    augmented_texts_train['actions_pseudo_label'] = augmented_texts_train['actions_pseudo_label'].apply(lambda x: [x])\n",
    "    augmented_texts_train = augmented_texts_train.merge(\n",
    "        transcripts_df.drop(['actions_pseudo_label', 'transcript_text', 'tokenized_text'], axis=1), \n",
    "        on='sample_id', \n",
    "        how='inner'\n",
    "    )\n",
    "    return pd.concat([transcripts_df, augmented_texts_train], ignore_index=True)\n",
    "\n",
    "\n",
    "def tfidf_detector_train(**kwargs):\n",
    "    train_df = kwargs['train_df']\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    \n",
    "    y = mlb.fit_transform(train_df['actions_pseudo_label'])\n",
    "    \n",
    "    train_df['tokenized_text_str'] = train_df['tokenized_text'].apply(lambda x: ' '.join(ast.literal_eval(x)) if isinstance(x, str) else ' '.join(x))\n",
    "    train_df['tokenized_events_str'] = train_df['events'].apply(lambda x: ' '.join([(i or '') for i in (ast.literal_eval(x) if isinstance(x, str) else x)]))\n",
    "\n",
    "    get_text = FunctionTransformer(lambda X: X['tokenized_text_str'], validate=False)\n",
    "    get_events = FunctionTransformer(lambda X: X['tokenized_events_str'], validate=False)\n",
    "    sublinear_tf = True\n",
    "    \n",
    "    solver = 'liblinear'\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "       ('features', FeatureUnion([\n",
    "           ('word_tfidf_text', Pipeline([('sel', get_text), ('tfidf', TfidfVectorizer(ngram_range=(1,3), analyzer='word', sublinear_tf=sublinear_tf))])),\n",
    "           ('char_tfidf_text', Pipeline([('sel', get_text), ('tfidf', TfidfVectorizer(ngram_range=(2,3), analyzer='char', sublinear_tf=sublinear_tf))])),\n",
    "           ('word_tfidf_events', Pipeline([('sel', get_events), ('tfidf', TfidfVectorizer(ngram_range=(1,2), analyzer='word', sublinear_tf=sublinear_tf))])),\n",
    "       ])),\n",
    "       ('classifier', MultiOutputClassifier(LogisticRegression(class_weight='balanced', solver=solver, max_iter=2000, C=0.5)))\n",
    "    ])\n",
    "    X = train_df[['tokenized_text_str','tokenized_events_str']]\n",
    "    pipeline.fit(X, y)\n",
    "    return pipeline, mlb\n",
    "\n",
    "\n",
    "def tfidf_detector_predict(pipeline, mlb, test_df, predict_threshold=0.6):\n",
    "    predictions_proba = pipeline.predict_proba(test_df)\n",
    "    proba_of_action = np.column_stack([pred[:, 1] for pred in predictions_proba])\n",
    "    proba_of_action_max = np.zeros_like(proba_of_action)\n",
    "    row_max_indices = proba_of_action.argmax(axis=1)\n",
    "    proba_of_action_max[np.arange(proba_of_action.shape[0]), row_max_indices] = proba_of_action[np.arange(proba_of_action.shape[0]), row_max_indices]\n",
    "    predictions = (proba_of_action_max > predict_threshold).astype(int)\n",
    "    output = [sublist[0] if any(sublist) else None for sublist in mlb.inverse_transform(predictions)]\n",
    "    return output\n",
    "\n",
    "def string_matching_prediction(**kwargs):\n",
    "    test_df = kwargs['test_df']\n",
    "    return test_df['actions_str_detected'].tolist()\n",
    "\n",
    "\n",
    "class Detector:\n",
    "\n",
    "    def prepare_df(self, transcripts_df):\n",
    "        transcripts_df = clean_events(transcripts_df)\n",
    "        transcripts_df['tokenized_event_name'] = transcripts_df['EventNameCleaned'].apply(lambda x: lemmatize(x, phrases_patterns))\n",
    "        transcripts_df['tokenized_text'] = transcripts_df['Text'].apply(lambda x: lemmatize(x, phrases_patterns))\n",
    "        transcripts_df['actions_in_text'] = transcripts_df['tokenized_text'].apply(extract_actions_from_lemmatized)\n",
    "        return transcripts_df\n",
    "        \n",
    "\n",
    "    def fit(self, transcripts_df):\n",
    "        transcripts_df = transcripts_df.copy()\n",
    "        with open('data/actions_processed_to_action.json', 'r') as f:\n",
    "            actions_map = json.load(f)\n",
    "        self.inverse_actions_map = {v: k for k, v in actions_map.items()}\n",
    "\n",
    "        transcripts_df = self.prepare_df(transcripts_df)\n",
    "        transcripts_df = pseudo_df.merge(transcripts_df, on='sample_id', how='inner').rename(columns={'action_detected': 'actions_pseudo_label', 'actions_in_text': 'actions_str_detected', 'Text': 'transcript_text'})\n",
    "        transcripts_df['actions_pseudo_labbel'] =  transcripts_df['actions_pseudo_label'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "        \n",
    "        transcripts_df['actions_str_detected'] = transcripts_df['actions_str_detected'].apply(\n",
    "            lambda x: [self.inverse_actions_map.get(a, a) for a in (ast.literal_eval(x) if isinstance(x, str) else x)])\n",
    "        transcripts_df = concat_augmentations_to_fold_df(transcripts_df)\n",
    "        self.tfidf_pipeline, self.mlb = tfidf_detector_train(train_df=transcripts_df)\n",
    "\n",
    "    def predict(self, transcripts_df):\n",
    "        transcripts_df = transcripts_df.copy()\n",
    "        transcripts_df = self.prepare_df(transcripts_df)\n",
    "        transcripts_df = transcripts_df.rename(columns={'actions_in_text': 'actions_str_detected'})\n",
    "        transcripts_df['tokenized_text_str'] = transcripts_df['tokenized_text'].apply(lambda x: ' '.join(ast.literal_eval(x)) if isinstance(x, str) else ' '.join(x))\n",
    "        transcripts_df['tokenized_events_str'] = transcripts_df['events'].apply(lambda x: ' '.join([(i or '') for i in (ast.literal_eval(x) if isinstance(x, str) else x)]))\n",
    "        transcripts_df['actions_str_detected'] = transcripts_df['actions_str_detected'].apply(\n",
    "            lambda x: [self.inverse_actions_map.get(a, a) for a in (ast.literal_eval(x) if isinstance(x, str) else x)])\n",
    "        predicted_labels_tfidf = tfidf_detector_predict(self.tfidf_pipeline, self.mlb, transcripts_df)\n",
    "        predicted_labels_string_match = transcripts_df['actions_str_detected'].tolist()\n",
    "        predicted_labels_string_match = [p[0] if p else None for p in predicted_labels_string_match]\n",
    "        combined_labels = [\n",
    "        t if s is None else s\n",
    "        for s, t in zip(predicted_labels_string_match, predicted_labels_tfidf)]\n",
    "        transcripts_df['action_predicted'] = combined_labels\n",
    "        origin_mapping = {v: k for k, v in self.inverse_actions_map.items()}\n",
    "        transcripts_df['action_predicted_tokenized'] = transcripts_df['action_predicted'].apply(lambda a: origin_mapping.get(a, a))\n",
    "        return transcripts_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tfidf_validation_train(train_df):\n",
    "    y = train_df['Label'].values\n",
    "\n",
    "    get_text = FunctionTransformer(lambda X: X['tokenized_text_str'], validate=False)\n",
    "    get_events = FunctionTransformer(lambda X: X['tokenized_events_str'], validate=False)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('word_tfidf_text', Pipeline([('sel', get_text), ('tfidf', TfidfVectorizer(ngram_range=(1,3), analyzer='word'))])),\n",
    "            ('char_tfidf_text', Pipeline([('sel', get_text), ('tfidf', TfidfVectorizer(ngram_range=(2,4), analyzer='char'))])),\n",
    "            ('word_tfidf_events', Pipeline([('sel', get_events), ('tfidf', TfidfVectorizer(ngram_range=(1,2), analyzer='word'))])),\n",
    "        ])),\n",
    "        ('classifier', LogisticRegression(class_weight='balanced', solver='liblinear', max_iter=2000, C=1))\n",
    "    ])\n",
    "    X = train_df[['tokenized_text_str', 'tokenized_events_str']]\n",
    "    pipeline.fit(X, y)\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def tfidf_validator_predict(pipeline, test_df, predict_threshold=0.6):\n",
    "    X_test = test_df[['tokenized_text_str', 'tokenized_events_str']]\n",
    "    predictions_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (predictions_proba > predict_threshold).astype(int)\n",
    "    return y_pred\n",
    "\n",
    "class Validator:\n",
    "    def prepare_df(self, transcripts_df):\n",
    "        transcripts_df['tokenized_text_str'] = transcripts_df['tokenized_text'].apply(lambda x: ' '.join(ast.literal_eval(x)) if isinstance(x, str) else ' '.join(x))\n",
    "        transcripts_df['tokenized_events_str'] = transcripts_df['events'].apply(lambda x: ' '.join(['_'.join((w or '').split()) if w else '' for w in (ast.literal_eval(x) if isinstance(x, str) else x)]))\n",
    "        return transcripts_df\n",
    "        \n",
    "\n",
    "    def fit(self, transcripts_df):\n",
    "        transcripts_df = transcripts_df.copy()\n",
    "        transcripts_df = self.prepare_df(transcripts_df)\n",
    "        self.tfidf_pipeline = tfidf_validation_train(train_df=transcripts_df)\n",
    "\n",
    "    def predict(self, transcripts_df):\n",
    "        transcripts_df = transcripts_df.copy()\n",
    "        transcripts_df = self.prepare_df(transcripts_df)\n",
    "        y_pred = tfidf_validator_predict(self.tfidf_pipeline, transcripts_df)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "detector_trainer = Detector()\n",
    "detector_trainer.fit(transcripts_df)\n",
    "transcripts_train = detector_trainer.predict(transcripts_df)\n",
    "\n",
    "validator_trainer = Validator()\n",
    "validator_trainer.fit(transcripts_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventName</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Missed Shot by Darren CollisonRebound by Joel ...</td>\n",
       "      <td>If you go into that defensive circle and post ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           EventName  \\\n",
       "0  Missed Shot by Darren CollisonRebound by Joel ...   \n",
       "\n",
       "                                                Text  \n",
       "0  If you go into that defensive circle and post ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventName</th>\n",
       "      <th>Text</th>\n",
       "      <th>action_predicted</th>\n",
       "      <th>action_validity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Missed Shot by Darren CollisonRebound by Joel ...</td>\n",
       "      <td>If you go into that defensive circle and post ...</td>\n",
       "      <td>post up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           EventName  \\\n",
       "0  Missed Shot by Darren CollisonRebound by Joel ...   \n",
       "\n",
       "                                                Text action_predicted  \\\n",
       "0  If you go into that defensive circle and post ...          post up   \n",
       "\n",
       "   action_validity  \n",
       "0                0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_transcripts_df = pd.read_csv('data/transcripts.csv').drop('Label',axis = 1).head(1)\n",
    "display(new_transcripts_df)\n",
    "new_transcripts_df = detector_trainer.predict(new_transcripts_df)\n",
    "predicted_validation = validator_trainer.predict(new_transcripts_df)\n",
    "new_transcripts_df['action_validity'] = predicted_validation\n",
    "new_transcripts_df = new_transcripts_df[['EventName', 'Text', 'action_predicted', 'action_validity']]\n",
    "display(new_transcripts_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsc_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
