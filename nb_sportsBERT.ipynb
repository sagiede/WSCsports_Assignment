{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WSC Project - Data Analysis & NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import ast\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# Data science imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP imports\n",
    "import nltk\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import json\n",
    "import os \n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer, MultiLabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.svm import LinearSVC\n",
    "import tiktoken\n",
    "import pickle\n",
    "# import xgboost as xgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Jupyter settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings for full text\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = list(set(pd.read_csv(\"data/actions.csv\")[\"parameter\"]))\n",
    "pseudo_df = pd.read_csv(\"data/pseudo_actions_labels_with_id.csv\")[['sample_id', 'action_detected']]\n",
    "transcripts_folds_df = pd.read_csv('data/transcripts_folds.csv')\n",
    "features_df = transcripts_folds_df[['sample_id', 'Text', 'tokenized_text', 'events', 'actions_in_text', 'fold1', 'fold2', 'fold3', 'fold4', 'fold5']]\n",
    "# features_df = transcripts_folds_df[['sample_id', 'Text', 'tokenized_text', 'events', 'actions_in_text']]\n",
    "\n",
    "expr_df = pseudo_df.merge(features_df, on='sample_id', how='inner').rename(columns={'action_detected': 'actions_pseudo_label', 'actions_in_text': 'actions_str_detected', 'Text': 'transcript_text'})\n",
    "expr_df['actions_pseudo_label'] =  expr_df['actions_pseudo_label'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "with open('data/actions_processed_to_action.json', 'r') as f:\n",
    "    actions_map = json.load(f)\n",
    "\n",
    "inverse_actions_map = {v: k for k, v in actions_map.items()}\n",
    "expr_df['actions_str_detected'] = expr_df['actions_str_detected'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "expr_df['actions_str_detected'] = expr_df['actions_str_detected'].apply(lambda actions: [inverse_actions_map.get(a, a) for a in actions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "augmented_texts_df = pd.read_csv('data/augmented_texts_processed.csv')[['sample_id', 'augmented_text', 'tokenized_augmented_text', 'action']]\n",
    "\n",
    "def concat_augmentations_to_fold_df(fold_train_df):\n",
    "    augmented_texts_train = augmented_texts_df[augmented_texts_df['sample_id'].isin(fold_train_df['sample_id'])]\n",
    "    augmented_texts_train = augmented_texts_train.rename(columns={'augmented_text': 'transcript_text', 'action': 'actions_pseudo_label', 'tokenized_augmented_text': 'tokenized_text'})\n",
    "    augmented_texts_train['actions_pseudo_label'] = augmented_texts_train['actions_pseudo_label'].apply(lambda x: [x])\n",
    "    augmented_texts_train = augmented_texts_train.merge(\n",
    "        fold_train_df.drop(['actions_pseudo_label', 'transcript_text', 'tokenized_text'], axis=1), \n",
    "        on='sample_id', \n",
    "        how='inner'\n",
    "    )\n",
    "    return pd.concat([fold_train_df, augmented_texts_train], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUGMENT_DATA = False\n",
    "INCLUDE_EVENTS = False\n",
    "\n",
    "df = expr_df.copy()\n",
    "action_counts = df['actions_pseudo_label'].explode().value_counts()\n",
    "\n",
    "fold_col = f'fold1'\n",
    "train_df = df[df[fold_col] == 'train']\n",
    "if AUGMENT_DATA:\n",
    "    train_df = concat_augmentations_to_fold_df(train_df)\n",
    "    val_df = df[df[fold_col] == 'val']\n",
    "\n",
    "if INCLUDE_EVENTS:\n",
    "    train_df['events_str'] = ['Events: ' + ', '.join(l) for l in [[] if l == '[None]' else eval(l) for l in train_df['events'].tolist()]]\n",
    "    train_df['events_and_transcript'] = train_df['events_str'] + '\\n Transcript: ' + train_df['transcript_text']\n",
    "else:\n",
    "    train_df['events_and_transcript'] =  train_df['transcript_text']\n",
    "\n",
    "if INCLUDE_EVENTS:\n",
    "    val_df['events_str'] = ['Events: ' + ', '.join(l) for l in [[] if l == '[None]' else eval(l) for l in val_df['events'].tolist()]]\n",
    "    val_df['events_and_transcript'] = val_df['events_str'] + '\\n Transcript: ' + val_df['transcript_text']\n",
    "else:\n",
    "    val_df['events_and_transcript'] =  val_df['transcript_text']\n",
    "\n",
    "val_df['events_and_transcript'] =  val_df['transcript_text']\n",
    "\n",
    "X_train = train_df['events_and_transcript'].tolist()\n",
    "y_train = train_df['actions_pseudo_label'].tolist()\n",
    "\n",
    "X_val = val_df['events_and_transcript'].tolist()\n",
    "y_val = val_df['actions_pseudo_label'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE_COLLECTION = 'train'\n",
    "INFERENCE_COLLECTION = 'val'\n",
    "\n",
    "EMBEDDINGS_TASK = 'detector'\n",
    "# EMBEDDINGS_TASK = 'validator'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_SPORTSBERT_NAME = 'SportsBERT'\n",
    "# MODEL_SPORTSBERT = f'microsoft/{MODEL_SPORTSBERT_NAME}'\n",
    "\n",
    "MODEL_BGE_NAME = 'bge-small-en-v1.5'\n",
    "MODEL_BGE = f'BAAI/{MODEL_BGE_NAME}'\n",
    "\n",
    "# MODEL, REV = MODEL_SPORTSBERT, \"refs/pr/4\"\n",
    "MODEL, MODEL_NAME, REV = MODEL_BGE, MODEL_BGE_NAME, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_deterministic(s=42):\n",
    "    os.environ[\"PYTHONHASHSEED\"]=str(s); random.seed(s); np.random.seed(s); torch.manual_seed(s)\n",
    "    try: torch.use_deterministic_algorithms(True)\n",
    "    except: pass\n",
    "set_deterministic(42)\n",
    "\n",
    "def build_train_pairs(X,Y,A):\n",
    "    T,A2,L=[],[],[]\n",
    "    for t,ys in zip(X,Y):\n",
    "        P=list(set(ys)); N=[a for a in A if a not in P]; k=max(len(ys),len(P))\n",
    "        if N: N=rng.choice(N,size=min(k,len(N)),replace=False).tolist()\n",
    "        for a in P: T.append(PREFIX + t); A2.append(H(a)); L.append(1.)\n",
    "        for a in N: T.append(PREFIX + t); A2.append(H(a)); L.append(0.)\n",
    "    return T,A2,torch.tensor(L,dtype=torch.float16)\n",
    "\n",
    "def build_val_pairs(X,Y,A):\n",
    "    T,A2,L=[],[],[]\n",
    "    for t,ys in zip(X,Y):\n",
    "        S=set(ys)\n",
    "        for a in A:\n",
    "            T.append(PREFIX + t)\n",
    "            A2.append(H(a))\n",
    "            L.append(1. if a in S else 0.)\n",
    "    return T,A2,torch.tensor(L,dtype=torch.float16)\n",
    "\n",
    "\n",
    "class Pairs(torch.utils.data.Dataset):\n",
    "    def __init__(self,tok,T,A,L,m=160): self.tok,tok.model_max_length,self.T,self.A,self.L,self.m=tok,m,T,A,L,m\n",
    "    def __len__(self): return len(self.T)\n",
    "    def __getitem__(self,i):\n",
    "        enc=self.tok(self.T[i], self.A[i], truncation=True, padding=\"max_length\", max_length=self.m)\n",
    "        # --- drop token_type_ids for single-segment models ---\n",
    "        enc.pop(\"token_type_ids\", None)  # <<< changed\n",
    "        enc={k:torch.tensor(v) for k,v in enc.items()}; enc[\"labels\"]=self.L[i]; return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train trainscripts data 222\n",
      "Val trainscripts data 385\n",
      "Train data points 450\n",
      "Val data points 14630\n",
      "14630\n",
      "val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/58 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/58 [00:02<02:01,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/58 [00:04<02:06,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 3/58 [00:07<02:11,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 4/58 [00:09<02:14,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 5/58 [00:12<02:09,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 6/58 [00:14<02:01,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 7/58 [00:16<01:54,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 8/58 [00:18<01:57,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 9/58 [00:21<01:59,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 10/58 [00:23<01:56,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 11/58 [00:26<01:54,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 12/58 [00:29<02:06,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 13/58 [00:33<02:13,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 14/58 [00:35<02:04,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 15/58 [00:38<02:05,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 16/58 [00:41<01:56,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 17/58 [00:43<01:49,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 18/58 [00:45<01:36,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 19/58 [00:47<01:33,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 20/58 [00:49<01:28,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 21/58 [00:52<01:27,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 22/58 [00:55<01:30,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 23/58 [00:57<01:27,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 24/58 [01:00<01:24,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 25/58 [01:02<01:19,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 26/58 [01:04<01:13,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 27/58 [01:06<01:07,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 28/58 [01:08<01:04,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 29/58 [01:11<01:07,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 30/58 [01:13<01:07,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 31/58 [01:15<00:59,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 32/58 [01:17<00:58,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 33/58 [01:21<01:03,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 34/58 [01:23<00:56,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 35/58 [01:24<00:50,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 36/58 [01:27<00:48,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 37/58 [01:29<00:47,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 38/58 [01:31<00:43,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 39/58 [01:33<00:40,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 40/58 [01:35<00:36,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 41/58 [01:37<00:34,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 42/58 [01:40<00:35,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 43/58 [01:42<00:33,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 44/58 [01:44<00:31,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 45/58 [01:47<00:30,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 46/58 [01:49<00:28,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 47/58 [01:53<00:29,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 48/58 [01:55<00:27,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 49/58 [01:58<00:24,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 50/58 [02:02<00:23,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 51/58 [02:05<00:22,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 52/58 [02:08<00:17,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 53/58 [02:10<00:14,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 54/58 [02:13<00:10,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 55/58 [02:16<00:08,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 56/58 [02:18<00:05,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 57/58 [02:21<00:02,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [02:22<00:00,  2.45s/it]\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cpu'\n",
    "\n",
    "if EMBEDDINGS_TASK == 'detector':\n",
    "    if MODEL == MODEL_BGE:\n",
    "        H = lambda action: f\". This commentary mentioned {action}.\"\n",
    "        PREFIX = 'Represent this sentence for classification: '\n",
    "    if MODEL == MODEL_SPORTSBERT:\n",
    "        H = lambda action: f\"This commentary mentions {action}.\"\n",
    "        PREFIX = ''\n",
    "    \n",
    "    H = lambda action: f\"This commentary mentions {action}.\"\n",
    "elif EMBEDDINGS_TASK == 'validator':\n",
    "    H = lambda action: f\"The {action} executed in this current play.\"\n",
    "\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL, revision=REV, use_fast=True)\n",
    "enc = AutoModel.from_pretrained(MODEL, revision=REV)\n",
    "enc.eval()\n",
    "\n",
    "\n",
    "Ttr,Atr,Ltr=build_train_pairs(X_train,y_train,ACTIONS)\n",
    "Tva,Ava,Lva=build_val_pairs(X_val,y_val,ACTIONS)\n",
    "ds_tr,ds_va=Pairs(tok,Ttr,Atr,Ltr),Pairs(tok,Tva,Ava,Lva)\n",
    "print('Train trainscripts data', len(X_train))\n",
    "print('Val trainscripts data', len(X_val))\n",
    "print('Train data points', len(Ttr))\n",
    "print('Val data points', len(Tva))\n",
    "\n",
    "if INFERENCE_COLLECTION =='train':\n",
    "    x_text = Ttr\n",
    "    x_acts = Atr\n",
    "    X_binary_label = Ltr\n",
    "\n",
    "if INFERENCE_COLLECTION =='val':\n",
    "    x_text = Tva\n",
    "    x_acts = Ava\n",
    "    X_binary_label = Lva\n",
    "print(len(x_text))\n",
    "\n",
    "print(INFERENCE_COLLECTION)\n",
    "\n",
    "\n",
    "def get_mean_pooled(out, encoded_input, model_type=MODEL):\n",
    "    if model_type == MODEL_SPORTSBERT:\n",
    "        attn_mask = encoded_input['attention_mask'].unsqueeze(-1)\n",
    "        hidden_attention = (out.last_hidden_state * attn_mask)\n",
    "    else:\n",
    "        hidden_attention = out.last_hidden_state\n",
    "        attn_mask = encoded_input['attention_mask'].unsqueeze(-1)\n",
    "    sum_hidden = hidden_attention.sum(1)\n",
    "    lens = attn_mask.sum(1).clamp(min=1)\n",
    "    mean_pooled = sum_hidden / lens\n",
    "    return mean_pooled\n",
    "\n",
    "mean_pooling_embs = []\n",
    "cls_embs = []\n",
    "bs = 256\n",
    "for i in tqdm(range(0, len(x_text), bs)):\n",
    "    \n",
    "    print(f\"{(i/len(x_text)*100):.1f}%\")\n",
    "    t_batch = x_text[i:i+bs]\n",
    "    a_batch = x_acts[i:i+bs]\n",
    "    \n",
    "    encoded_input = tok(t_batch, a_batch, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = enc(**encoded_input)\n",
    "        mean_pooled = get_mean_pooled(model_output, encoded_input, MODEL)\n",
    "        cls = model_output.last_hidden_state[:,0]\n",
    "        \n",
    "    mean_pooled = torch.nn.functional.normalize(mean_pooled, p=2, dim=1)\n",
    "    cls = torch.nn.functional.normalize(cls, p=2, dim=1)\n",
    "        \n",
    "    mean_pooling_embs.append(mean_pooled.detach().cpu())\n",
    "    cls_embs.append(cls.detach().cpu())\n",
    "cls_embs = torch.cat(cls_embs)\n",
    "mean_pooling_embs = torch.cat(mean_pooling_embs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/embds_bge-small-en-v1.5', 'val_embds_detector.pt')"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir = f'data/embds_{MODEL_NAME}'\n",
    "out_path = out_dir, f\"{INFERENCE_COLLECTION}_embds_{EMBEDDINGS_TASK}.pt\"\n",
    "out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/embds_bge-small-en-v1.5/val_embds_detector.pt\n"
     ]
    }
   ],
   "source": [
    "WRITE = True\n",
    "\n",
    "\n",
    "def save_embeddings(INFERENCE_COLLECTION, x_acts, cls_embs, mean_pooling_embs):\n",
    "\n",
    "    out_dir = f'data/embds_{MODEL_NAME}'\n",
    "    out_path = os.path.join(out_dir, f\"{INFERENCE_COLLECTION}_embds_{EMBEDDINGS_TASK}.pt\")\n",
    "    # Ensure the output directory exists\n",
    "    \n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare data to save\n",
    "    embd_data = {\n",
    "        \"INFERENCE_COLLECTION\": INFERENCE_COLLECTION,\n",
    "        \"actions_labels\": [s.replace(\"This commentary mentions \", \"\").rstrip(\".\") for s in x_acts],\n",
    "        \"cls_embs\": cls_embs.cpu(),  # ensure on CPU\n",
    "        \"mean_pooling_embs\": mean_pooling_embs.cpu(),\n",
    "        \"sample_label\": X_binary_label.cpu()\n",
    "\n",
    "\n",
    "    }\n",
    "\n",
    "    # Save as .pt (PyTorch native)\n",
    "    torch.save(embd_data, out_path)\n",
    "\n",
    "    # For pickle, better to convert to numpy for portability\n",
    "    embd_data_np = {**embd_data,\n",
    "                    \"cls_embs\": cls_embs.cpu().numpy(),\n",
    "                    \"mean_pooling_embs\": mean_pooling_embs.cpu().numpy()}\n",
    "\n",
    "    with open(os.path.join(out_dir, f\"{INFERENCE_COLLECTION}_embds_{EMBEDDINGS_TASK}.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(embd_data_np, f)\n",
    "\n",
    "if WRITE:\n",
    "    save_embeddings(INFERENCE_COLLECTION, x_acts, cls_embs, mean_pooling_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(INFERENCE_COLLECTION, EMBEDDINGS_TASK):\n",
    "    out_dir = f'data/embds_{MODEL_NAME}'\n",
    "    out_path = os.path.join(out_dir, f\"{INFERENCE_COLLECTION}_embds_{EMBEDDINGS_TASK}.pt\")\n",
    "    return torch.load(out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = load_embeddings('train', EMBEDDINGS_TASK)\n",
    "val_embeddings = load_embeddings('val', EMBEDDINGS_TASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMBDS_COL = 'cls_embs'\n",
    "EMBDS_COL = 'mean_pooling_embs'\n",
    "# EMBDS_COL = 'both'\n",
    "\n",
    "train_binary_labels = train_embeddings['sample_label']\n",
    "if EMBDS_COL == 'both':\n",
    "    train_embds = np.concatenate(\n",
    "        [train_embeddings['cls_embs'], train_embeddings['mean_pooling_embs']], axis=1\n",
    "    )\n",
    "    test_embds = np.concatenate(\n",
    "        [val_embeddings['cls_embs'], val_embeddings['mean_pooling_embs']], axis=1\n",
    "    )\n",
    "else:\n",
    "    train_embds = train_embeddings[EMBDS_COL]\n",
    "    test_embds = val_embeddings[EMBDS_COL]\n",
    "\n",
    "test_binary_labels = val_embeddings['sample_label']\n",
    "\n",
    "train_actions_labels = train_embeddings['actions_labels']\n",
    "test_actions_labels = val_embeddings['actions_labels']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.953 0.547 0.695\n",
      "Train metrics: \t\t Precision: 0.953 \t Recall: 0.547 \t F1: 0.695\n",
      "Validation metrics: \t Precision: 0.235 \t Recall: 0.518 \t F1: 0.323\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def embd_action_prediction(train_embds, train_binary_labels, test_embds, predict_threshold=0.7, C=1.0):\n",
    "    # Explicitly set penalty to 'l2' for L2 regularization\n",
    "    \n",
    "    pca = PCA(n_components=min(100, train_embds.shape[1]))\n",
    "    train_embds = pca.fit_transform(train_embds)\n",
    "    test_embds = pca.transform(test_embds)\n",
    "    # clf = LGBMClassifier(\n",
    "    #     n_estimators=50,\n",
    "    #     max_depth=4,\n",
    "    #     num_threads=1,\n",
    "    #     random_state=42,\n",
    "    #     verbose=-1,\n",
    "    #     reg_alpha=5.0,   # Increased L1 regularization\n",
    "    #     reg_lambda=5.0,  # Increased L2 regularization\n",
    "    #     subsample=0.2    # Subsample 70% of data for each tree\n",
    "    # )\n",
    "    clf = LogisticRegression(penalty='l2', C=0.01, solver='liblinear', random_state=42)  # Increased regularization (smaller C)\n",
    "    clf.fit(train_embds, train_binary_labels)\n",
    "    train_proba = clf.predict_proba(train_embds)[:,1]\n",
    "    test_proba = clf.predict_proba(test_embds)[:,1]\n",
    "\n",
    "    train_preds = (train_proba > predict_threshold).astype(int)\n",
    "    test_preds = (test_proba > predict_threshold).astype(int)\n",
    "    return train_preds, test_preds\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(np.concatenate([train_embds, test_embds], axis=0))\n",
    "train_embds = scaler.transform(train_embds)\n",
    "test_embds = scaler.transform(test_embds)\n",
    "\n",
    "train_preds, test_preds = embd_action_prediction(train_embds, train_binary_labels, test_embds)\n",
    "\n",
    "\n",
    "precision_train = precision_score(train_binary_labels, train_preds)\n",
    "recall_train = recall_score(train_binary_labels, train_preds)\n",
    "f1_train = f1_score(train_binary_labels, train_preds)\n",
    "print('train', round(precision_train, 3), round(recall_train, 3), round(f1_train, 3))\n",
    "print('Train metrics: \\t\\t Precision: {} \\t Recall: {} \\t F1: {}'.format(round(precision_train, 3), round(recall_train, 3), round(f1_train, 3)))\n",
    "\n",
    "# np.random.shuffle(test_binary_labels)\n",
    "precision_test = precision_score(test_binary_labels, test_preds)\n",
    "recall_test = recall_score(test_binary_labels, test_preds)\n",
    "f1_test = f1_score(test_binary_labels, test_preds)\n",
    "print('Validation metrics: \\t Precision: {} \\t Recall: {} \\t F1: {}'.format(round(precision_test, 3), round(recall_test, 3), round(f1_test, 3)))\n",
    "\n",
    "\n",
    "\n",
    "# Calculate precision and recall per action label in train set\n",
    "def per_action_metrics(actions_labels, binary_labels, preds):\n",
    "    actions = set(actions_labels)\n",
    "    action_metrics = {}\n",
    "    for action in actions:\n",
    "        mask = [a == action for a in actions_labels]\n",
    "        y_true = [binary_labels[i] for i, m in enumerate(mask) if m]\n",
    "        y_pred = [preds[i] for i, m in enumerate(mask) if m]\n",
    "        if len(y_true) == 0:\n",
    "            continue\n",
    "        prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "        rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "        action_metrics[action] = (round(prec, 3), round(rec, 3))\n",
    "    for action, (prec, rec) in action_metrics.items():\n",
    "        print(f\"Action: {action:20s} Precision: {prec:.3f} Recall: {rec:.3f}\")\n",
    "\n",
    "# per_action_metrics(train_actions_labels, train_binary_labels, train_preds)\n",
    "# per_action_metrics(test_actions_labels, test_binary_labels, test_preds)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SportsBERT Action detection results\n",
      "\n",
      "train 0.858 0.864 0.861\n",
      "Train metrics: \t\t Precision: 0.858 \t Recall: 0.864 \t F1: 0.861\n",
      "Validation metrics: \t Precision: 0.102 \t Recall: 0.81 \t F1: 0.181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('SportsBERT Action detection results')\n",
    "print(\"\"\"\n",
    "train 0.858 0.864 0.861\n",
    "Train metrics: \t\t Precision: 0.858 \t Recall: 0.864 \t F1: 0.861\n",
    "Validation metrics: \t Precision: 0.102 \t Recall: 0.81 \t F1: 0.181\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsc_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
