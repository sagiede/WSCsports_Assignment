{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E2E Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# classifier = pipeline(\"text-classification\", model=\"sumit2603/bert-sports-interview-classifier\")\n",
    "from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoTokenizer, AutoModelForQuestionAnswering, AutoConfig\n",
    "from transformers import pipeline, TrainerCallback, TrainingArguments, TrainerControl, TrainerState, Trainer\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "# from peft import get_peft_model, LoraConfig, TaskType\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
    "\n",
    "\n",
    "# Check if GPU is available\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"      # hard-disable\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"      # belt & suspenders\n",
    "# (optional) keep runs clean:\n",
    "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcripts_df = pd.read_csv('data/transcripts.csv').drop_duplicates(ignore_index=True)\n",
    "# transcripts_df.insert(0, 'sample_id', range(1, len(transcripts_df) + 1))\n",
    "\n",
    "# ACTIONS = list(set(pd.read_csv(\"data/actions.csv\")[\"parameter\"]))\n",
    "\n",
    "# pseudo_df = pd.read_csv(\"data/pseudo_actions_labels_with_id.csv\")[['sample_id', 'action_detected']]\n",
    "transcripts_df = pd.read_csv('data/transcrpits_processed.csv').drop_duplicates(ignore_index=True)\n",
    "augmented_texts_df = pd.read_csv('data/augmented_texts_processed.csv')[['sample_id', 'augmented_text', 'tokenized_augmented_text', 'action']]\n",
    "transcripts_folds_df = pd.read_csv('data/transcripts_folds.csv')\n",
    "\n",
    "train_ids = transcripts_folds_df[transcripts_folds_df['fold1'] == 'val'][['sample_id']] # val in origin was 40%, now i use it for train\n",
    "val_ids = transcripts_folds_df[transcripts_folds_df['fold1'] == 'train'][['sample_id']] # val in origin was 20%, now i use it for val\n",
    "test_ids = transcripts_folds_df[transcripts_folds_df['fold1'] == 'test'][['sample_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# transcripts_train, transcripts_test = train_test_split(transcripts_df, test_size=0.6, random_state=42)\n",
    "# transcripts_val, transcripts_test = train_test_split(transcripts_test, test_size=0.66, random_state=42)\n",
    "\n",
    "transcripts_train = transcripts_df.merge(train_ids, on='sample_id')\n",
    "transcripts_val = transcripts_df.merge(val_ids, on='sample_id')\n",
    "transcripts_test = transcripts_df.merge(test_ids, on='sample_id')\n",
    "\n",
    "transcripts_train_aug = augmented_texts_df.merge(transcripts_train[['sample_id']], on='sample_id', how='inner').merge(transcripts_df, on='sample_id', how='inner')[['sample_id', 'Text', 'events', 'Label']]\n",
    "\n",
    "transcripts_train_aug = transcripts_train_aug[['Text', 'events', 'Label']]\n",
    "transcripts_train = transcripts_train[['Text', 'events', 'Label']]\n",
    "transcripts_test = transcripts_test[['Text', 'events', 'Label']]\n",
    "transcripts_val = transcripts_val[['Text', 'events', 'Label']]\n",
    "\n",
    "train_neg = transcripts_train[transcripts_train['Label'] == 0]\n",
    "train_neg_aug = transcripts_train_aug[transcripts_train_aug['Label'] == 0]\n",
    "train_neg = (pd.concat([train_neg, train_neg, train_neg, train_neg_aug, train_neg_aug], ignore_index=True))\n",
    "\n",
    "train_pos = transcripts_train[transcripts_train['Label'] == 1]\n",
    "train_pos_aug = transcripts_train_aug[transcripts_train_aug['Label'] == 1].sample(frac=0.33, random_state=42)\n",
    "train_pos = (pd.concat([train_pos, train_pos_aug], ignore_index=True))\n",
    "\n",
    "train_df_balanced = pd.concat([train_pos, train_neg], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    import random\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def print_metrics(y_true, y_pred):\n",
    "  acc = accuracy_score(y_true, y_pred)\n",
    "  micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "  macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "  weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "  precision_label_0 = precision_score(y_true, y_pred, labels=[0], average='macro', zero_division=0)\n",
    "  precision_label_1 = precision_score(y_true, y_pred, labels=[1], average='macro', zero_division=0)\n",
    "  recall_label_0 = recall_score(y_true, y_pred, labels=[0], average='macro', zero_division=0)\n",
    "  recall_label_1 = recall_score(y_true, y_pred, labels=[1], average='macro', zero_division=0)\n",
    "  tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "  return pd.DataFrame({\n",
    "      \"precision_label_0\": [round(precision_label_0, 3)],\n",
    "      \"precision_label_1\": [round(precision_label_1, 3)],\n",
    "      \"recall_label_0\": [round(recall_label_0, 3)],\n",
    "      \"recall_label_1\": [round(recall_label_1, 3)],\n",
    "      \"micro_f1\": [round(micro_f1, 3)],\n",
    "      \"macro_f1\": [round(macro_f1, 3)],\n",
    "      \"true_negative\": [int(tn)],\n",
    "      \"false_positive\": [int(fp)],\n",
    "      \"false_negative\": [int(fn)],\n",
    "      \"true_positive\": [int(tp)]\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/SportsBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model_name = 'RajSang/pegasus-sports-titles'\n",
    "\n",
    "\n",
    "\n",
    "# model_name = \"Chrisneverdie/OnlySportsLM_196M\"\n",
    "# model_name = 'Chrisneverdie/OnlySports_Classifier'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model_name = 'emeraldgoose/bert-base-v1-sports'\n",
    "# grads = ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
    "\n",
    "# model_name = 'leomaurodesenv/bert-basketball-qa'\n",
    "# grads = ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
    "\n",
    "\n",
    "\n",
    "# model_name = 'MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli'\n",
    "\n",
    "# model_name = 'SushantGautam/SportsSum'\n",
    "# grads = ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
    "\n",
    "model_name = \"microsoft/SportsBERT\"\n",
    "grads = ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # or set to \"cpu\" to force CPU\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# config = LoraConfig(task_type=TaskType.SEQ_CLS, r=32, lora_alpha=32, lora_dropout=0.1)\n",
    "# model = get_peft_model(model, config)\n",
    "\n",
    "for name, p in model.named_parameters():\n",
    "    p.requires_grad = name in grads or name.startswith((\"bert.encoder.layer.11\"))\n",
    "    # p.requires_grad = name.startswith((\"classifier\", \"score\"))\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total     = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable params: {trainable}/{total} ({100*trainable/total:.2f}%)\")\n",
    "\n",
    "# Union transcripts_train and transcripts_train_aug for training dataset\n",
    "# train_df_union = train_df_balanced[:400]\n",
    "train_df_union = train_df_balanced\n",
    "train_dataset = Dataset.from_pandas(\n",
    "    train_df_union[['Text', 'Label']].rename(columns={'Text': 'text', 'Label': 'label'})\n",
    ")\n",
    "test_dataset = Dataset.from_pandas(transcripts_val[['Text', 'Label']].rename(columns={'Text': 'text', 'Label': 'label'}))\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"data/validator_checkpoint\",\n",
    "    report_to=\"none\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps = 2,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=2,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=2,\n",
    "    learning_rate=1e-4, \n",
    "    weight_decay=0.2,  # Further increased regularization\n",
    "    gradient_accumulation_steps=1,\n",
    "    lr_scheduler_type='polynomial',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "# Define compute_metrics function\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # Calculate accuracy\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    # Calculate micro F1\n",
    "    micro_f1 = f1_score(labels, predictions, average='micro')\n",
    "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
    "    weighted_f1 = f1_score(labels, predictions, average='weighted')\n",
    "    # Calculate precision for label 0 and label 1\n",
    "    from sklearn.metrics import precision_score, confusion_matrix, recall_score\n",
    "    precision_label_0 = precision_score(labels, predictions, labels=[0], average='macro', zero_division=0)\n",
    "    precision_label_1 = precision_score(labels, predictions, labels=[1], average='macro', zero_division=0)\n",
    "    recall_label_0 = recall_score(labels, predictions, labels=[0], average='macro', zero_division=0)\n",
    "    recall_label_1 = recall_score(labels, predictions, labels=[1], average='macro', zero_division=0)\n",
    "    # Calculate confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, predictions, labels=[0,1]).ravel()\n",
    "    return {\n",
    "        \"precision_label_0\": round(precision_label_0, 3),\n",
    "        \"precision_label_1\": round(precision_label_1, 3),\n",
    "        \"recall_label_0\": round(recall_label_0, 3),\n",
    "        \"recall_label_1\": round(recall_label_1, 3),\n",
    "        \"accuracy\": round(acc, 3),\n",
    "        \"micro_f1\": round(micro_f1, 3),\n",
    "        \"macro_f1\": round(macro_f1, 3),\n",
    "        \"weighted_f1\": round(weighted_f1, 3),\n",
    "        \"true_negative\": int(tn),\n",
    "        \"false_positive\": int(fp),\n",
    "        \"false_negative\": int(fn),\n",
    "        \"true_positive\": int(tp)\n",
    "    }\n",
    "\n",
    "\n",
    "class PrintLossCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Print train and eval loss at the end of each epoch\n",
    "        train_loss = state.log_history[-2]['loss'] if len(state.log_history) > 1 and 'loss' in state.log_history[-2] else None\n",
    "        eval_loss = state.log_history[-1]['eval_loss'] if len(state.log_history) > 0 and 'eval_loss' in state.log_history[-1] else None\n",
    "        print(f\"Epoch {int(state.epoch) if state.epoch is not None else '?'}: train_loss = {train_loss}, eval_loss = {eval_loss}\")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[PrintLossCallback()],\n",
    ")\n",
    "\n",
    "# Finetune the model\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Print the chosen/best checkpoint path\n",
    "if hasattr(trainer, 'state') and hasattr(trainer.state, 'best_model_checkpoint') and trainer.state.best_model_checkpoint is not None:\n",
    "    print(f\"Best model checkpoint: {trainer.state.best_model_checkpoint}\")\n",
    "else:\n",
    "    print(\"No best model checkpoint found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"text-classification\", model=trainer.model, tokenizer=tokenizer, device=0 if DEVICE == \"cuda\" else -1)\n",
    "test_dataset = Dataset.from_pandas(transcripts_test[['Text', 'Label']].rename(columns={'Text': 'text', 'Label': 'label'}))\n",
    "\n",
    "y_true = test_dataset['label']\n",
    "y_pred = [int(d['label']) if isinstance(d['label'], int) else int(d['label'].split('_')[-1]) for d in classifier(list(test_dataset['text']), batch_size=32)]\n",
    "print_metrics(y_true, y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsc_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
